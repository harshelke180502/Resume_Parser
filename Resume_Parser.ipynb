{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxU9kjYEbkirISNijwQCH+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshelke180502/Resume_Parser/blob/main/Resume_Parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfemsjXdckPa"
      },
      "outputs": [],
      "source": [
        "!pip install pdfplumber spacy python-docx nltk\n",
        "!pip install sentence-transformers\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "data = files.upload()\n"
      ],
      "metadata": {
        "id": "Tzs39cd0dbIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "  text=\" \"\n",
        "  with pdfplumber.open(pdf_path) as pdf:\n",
        "    for page in pdf.pages:\n",
        "      text+=page.extract_text()\n",
        "\n",
        "  return text\n",
        "\n"
      ],
      "metadata": {
        "id": "GgZPeEZMdbLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "huPdjxxU7jdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_text=extract_text_from_pdf(\"/content/Harsh_Shelke (1) (2).pdf\")"
      ],
      "metadata": {
        "id": "X--8ZxaFfdRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(resume_text)"
      ],
      "metadata": {
        "id": "B6qf8gRxffZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def clean_text(text):\n",
        "  text=re.sub(r'\\s+',' ',text)\n",
        "  return text.strip()\n",
        "\n",
        "cleaned_text=clean_text(resume_text)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OFyEIpDugHc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text"
      ],
      "metadata": {
        "id": "iIl7ZvspgSsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "NLP=spacy.load(\"en_core_web_sm\")\n",
        "doc=NLP(cleaned_text)\n",
        "def extract_entity(doc):\n",
        "  entities={\"PERSON\":[], \"ORG\":[], \"DATE\":[], \"GPE\":[]}\n",
        "  for ent in doc.ents:\n",
        "    if ent.label_ in entities:\n",
        "      entities[ent.label_].append(ent.text)\n",
        "  return entities\n",
        "\n",
        "entities=extract_entity(doc)\n",
        "entities\n",
        "\n"
      ],
      "metadata": {
        "id": "FwkhNZargz8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_contact_info(text):\n",
        "  email_id=re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',text)\n",
        "  phone_number=re.findall(r'\\d{3}-\\d{3}-\\d{4}',text)\n",
        "  linkedin=re.findall(r'linkedin.com/in/[a-zA-Z0-9-]+',text)\n",
        "  github=re.findall(r'github.com/[a-zA-Z0-9-]+',text)\n",
        "  return {\n",
        "      \"email_id\":email_id[0] if email_id else None,\n",
        "      \"phone_number\":phone_number[0] if phone_number else None,\n",
        "      \"linkedin\":linkedin[0] if linkedin else None,\n",
        "      \"github\":github[0] if github else None\n",
        "  }"
      ],
      "metadata": {
        "id": "u8cSCEXrhLcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contact=extract_contact_info(cleaned_text)"
      ],
      "metadata": {
        "id": "dGBVyhuihlPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_eductaion(text,section_name):\n",
        "  pattern=re.compile(rf\"{section_name}(.+?)(?=(Education|Experience|Technical Skills|Projects|$))\", re.IGNORECASE | re.DOTALL)\n",
        "  match=pattern.search(text)\n",
        "  return match.group(1).strip() if match else None\n",
        "education_txt=extract_eductaion(cleaned_text,\"Education\")\n",
        "experience_txt=extract_eductaion(cleaned_text,\"Experience\")\n",
        "skills_txt=extract_eductaion(cleaned_text,\"Technical Skills\")\n",
        "projects_txt=extract_eductaion(cleaned_text,\"Projects\")\n",
        "\n"
      ],
      "metadata": {
        "id": "FZzK79tWjCqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rAktVdtJpZWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skills_txt\n",
        "\n"
      ],
      "metadata": {
        "id": "yNC-5N5Nl4HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experience_txt"
      ],
      "metadata": {
        "id": "Dd93abK-megW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "projects_txt"
      ],
      "metadata": {
        "id": "7mvvp6EwpsXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_resume = {\n",
        "    \"name\": entities[\"PERSON\"][0] if entities[\"PERSON\"] else None,\n",
        "    \"email\": contact[\"email_id\"],\n",
        "    \"phone\": contact[\"phone_number\"],\n",
        "    \"linkedin\": contact[\"linkedin\"],\n",
        "    \"github\": contact[\"github\"],\n",
        "    \"education\": education_txt,\n",
        "    \"experience\": experience_txt,\n",
        "    \"technical_skills\": skills_txt,\n",
        "    \"projects\": projects_txt\n",
        "\n",
        "}\n",
        "\n",
        "import json\n",
        "print(json.dumps(parsed_resume, indent=4))"
      ],
      "metadata": {
        "id": "wdUrKr6QnDmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert\n",
        "!pip install sentence-transformers\n"
      ],
      "metadata": {
        "id": "Smhi1dkloGqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "## üîß **Importing Libraries**\n",
        "\n",
        "```python\n",
        "import pdfplumber\n",
        "import re\n",
        "import spacy\n",
        "from keybert import KeyBERT\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "```\n",
        "\n",
        "### üìå What These Do:\n",
        "\n",
        "* **`pdfplumber`**: Reads and extracts text from PDF files **page-by-page**.\n",
        "* **`re`**: Python‚Äôs **regular expression** module used for text pattern matching and cleaning.\n",
        "* **`spacy`**: An NLP library used for **text preprocessing**, named entity recognition (NER), etc.\n",
        "* **`KeyBERT`**: A keyword extraction model that uses **BERT-style embeddings**.\n",
        "* **`SentenceTransformer`**: Provides pre-trained transformer models for **sentence embeddings**.\n",
        "* **`util`**: Utilities for comparing embeddings, like **cosine similarity**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç **Model Initialization**\n",
        "\n",
        "```python\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
        "```\n",
        "\n",
        "### ‚úÖ Purpose:\n",
        "\n",
        "You're loading **pre-trained models** for different NLP tasks:\n",
        "\n",
        "* **`SentenceTransformer` model**: Converts any sentence or phrase into a **384-dimensional vector**. Useful for **semantic similarity**.\n",
        "\n",
        "  ‚úÖ Example:\n",
        "\n",
        "  ```python\n",
        "  model.encode(\"Machine learning with TensorFlow\")  \n",
        "  ‚Üí returns a dense vector: [0.03, -0.01, 0.12, ..., 0.06]\n",
        "  ```\n",
        "\n",
        "* **`spacy.load(\"en_core_web_sm\")`**: Loads SpaCy's **small English model**, which supports:\n",
        "\n",
        "  * Tokenization\n",
        "  * Part-of-speech tagging\n",
        "  * Named entity recognition (NER)\n",
        "\n",
        "* **`KeyBERT(...)`**: Uses the same transformer model (`MiniLM`) to extract the **most representative phrases** from any block of text.\n",
        "\n",
        "---\n",
        "\n",
        "## üìÑ **PDF Text Extraction**\n",
        "\n",
        "```python\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = ''\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + '\\n'\n",
        "    return text.strip()\n",
        "```\n",
        "\n",
        "### ‚úÖ What This Does:\n",
        "\n",
        "* Reads a PDF file **page-by-page**\n",
        "* Extracts **text content** from each page\n",
        "* Concatenates all the page texts into one string\n",
        "* Returns the cleaned, final **document text**\n",
        "\n",
        "### üß™ Example:\n",
        "\n",
        "Suppose `resume.pdf` has 2 pages:\n",
        "\n",
        "* Page 1:\n",
        "\n",
        "  ```\n",
        "  Harsh Shelke\n",
        "  Skills: Python, Flask, TensorFlow\n",
        "  ```\n",
        "* Page 2:\n",
        "\n",
        "  ```\n",
        "  Projects:\n",
        "  - Diabetic Retinopathy Detection\n",
        "  - Tour Package Prediction\n",
        "  ```\n",
        "\n",
        "Then calling:\n",
        "\n",
        "```python\n",
        "extract_text_from_pdf(\"resume.pdf\")\n",
        "```\n",
        "\n",
        "Will return:\n",
        "\n",
        "```text\n",
        "\"Harsh Shelke\\nSkills: Python, Flask, TensorFlow\\nProjects:\\n- Diabetic Retinopathy Detection\\n- Tour Package Prediction\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üßπ **Basic Cleanup Function**\n",
        "\n",
        "```python\n",
        "def clean_text(text):\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "```\n",
        "\n",
        "### ‚úÖ Purpose:\n",
        "\n",
        "* Removes **extra spaces**, tabs, or newlines from the text.\n",
        "* Makes everything into **one uniform block of text** with single spaces between words.\n",
        "\n",
        "### üîç Regex Explanation:\n",
        "\n",
        "* `\\s+`: Matches **one or more** whitespace characters (space, tab, newline).\n",
        "* `re.sub(..., ' ', ...)`: Replaces them all with a single space.\n",
        "* `.strip()`: Removes extra space from **start and end** of the text.\n",
        "\n",
        "### üß™ Example:\n",
        "\n",
        "```python\n",
        "text = \"Harsh   Shelke\\n\\nSkills:    Python,\\n\\tFlask\"\n",
        "clean_text(text)\n",
        "```\n",
        "\n",
        "Returns:\n",
        "\n",
        "```text\n",
        "\"Harsh Shelke Skills: Python, Flask\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üìë **Section Extraction Function**\n",
        "\n",
        "```python\n",
        "def extract_section(text, section_name):\n",
        "    pattern = re.compile(rf'{section_name}(.+?)(?=(Education|Work Experience|Techinal Skills|Projects|Certifications|$))',\n",
        "                         re.IGNORECASE | re.DOTALL)\n",
        "    match = pattern.search(text)\n",
        "    return match.group(1).strip() if match else \"\"\n",
        "```\n",
        "\n",
        "### ‚úÖ Purpose:\n",
        "\n",
        "To **extract the text under a specific section heading** (like `\"Projects\"`, `\"Education\"`, etc.) from the resume.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Regex Breakdown:\n",
        "\n",
        "Let‚Äôs say `section_name = \"Projects\"`\n",
        "\n",
        "```regex\n",
        "Projects(.+?)(?=(Education|Work Experience|Techinal Skills|Projects|Certifications|$))\n",
        "```\n",
        "\n",
        "* `Projects`: Starting keyword (passed dynamically)\n",
        "* `(.+?)`: **Non-greedy match** of all content after the section heading.\n",
        "* `(?=...)`: **Lookahead** to stop matching when the next section begins:\n",
        "\n",
        "  * `Education`, `Work Experience`, `Certifications`, etc.\n",
        "\n",
        "Also uses:\n",
        "\n",
        "* `re.IGNORECASE`: Case-insensitive matching\n",
        "* `re.DOTALL`: So `.` can match **newline characters** too.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ Example Resume Text:\n",
        "\n",
        "```text\n",
        "Education\n",
        "MIT World Peace University, B.Tech CS\n",
        "\n",
        "Projects\n",
        "- Heart Failure Prediction using XGBoost\n",
        "- Resume Parser using Python\n",
        "\n",
        "Certifications\n",
        "AWS Cloud Practitioner\n",
        "```\n",
        "\n",
        "Calling:\n",
        "\n",
        "```python\n",
        "extract_section(text, \"Projects\")\n",
        "```\n",
        "\n",
        "Returns:\n",
        "\n",
        "```text\n",
        "\"- Heart Failure Prediction using XGBoost\\n- Resume Parser using Python\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary Table\n",
        "\n",
        "| Function                | What It Does                     | Example Input                          | Example Output                |\n",
        "| ----------------------- | -------------------------------- | -------------------------------------- | ----------------------------- |\n",
        "| `extract_text_from_pdf` | Extracts text from all PDF pages | PDF file                               | Text content of entire resume |\n",
        "| `clean_text`            | Removes excessive spacing        | `\"Hello\\n  World\"`                     | `\"Hello World\"`               |\n",
        "| `extract_section`       | Gets content under a heading     | `\"Projects\\n- A\\n- B\\nCertifications\"` | `\"- A\\n- B\"`                  |\n",
        "\n"
      ],
      "metadata": {
        "id": "qZFKx_pgNE4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import re\n",
        "import spacy\n",
        "from keybert import KeyBERT\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load models\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
        "\n",
        "# ---------- PDF Text Extraction ----------\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = ''\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + '\\n'\n",
        "    return text.strip()\n",
        "\n",
        "# ---------- Basic Cleanup ----------\n",
        "def clean_text(text):\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "# ---------- Section Extraction ----------\n",
        "def extract_section(text, section_name):\n",
        "    pattern = re.compile(rf'{section_name}(.+?)(?=(Education|Work Experience|Techinal Skills|Projects|Certifications|$))',\n",
        "                         re.IGNORECASE | re.DOTALL)\n",
        "    match = pattern.search(text)\n",
        "    return match.group(1).strip() if match else \"\""
      ],
      "metadata": {
        "id": "cDwVe4xe5D6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=extract_text_from_pdf(\"/content/Harsh_Shelke (1) (2).pdf\")"
      ],
      "metadata": {
        "id": "vrKls_aJ64-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "id": "mQu1T6i-7oQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text=clean_text(text)\n",
        "cleaned_text"
      ],
      "metadata": {
        "id": "zCw5A35x7wUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_section(cleaned_text,\"Projects\")"
      ],
      "metadata": {
        "id": "Yh1FSWDr725n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Part 1: **Automatic Skill Extraction**\n",
        "\n",
        "```python\n",
        "def extract_skills_auto(text, top_n=15):\n",
        "    keywords = kw_model.extract_keywords(\n",
        "        text,\n",
        "        keyphrase_ngram_range=(1, 3),\n",
        "        stop_words='english',\n",
        "        top_n=top_n\n",
        "    )\n",
        "    return [kw[0] for kw in keywords]\n",
        "```\n",
        "\n",
        "### ‚úÖ Purpose:\n",
        "\n",
        "To automatically extract the **top `n` most relevant skills or key phrases** from a block of text (typically from a resume or job description) using **KeyBERT**, which is a BERT-based keyword extraction tool.\n",
        "\n",
        "### ‚öôÔ∏è How It Works:\n",
        "\n",
        "* **`kw_model.extract_keywords`** uses BERT-based embeddings to find keywords/phrases that are **semantically representative** of the text.\n",
        "* **`keyphrase_ngram_range=(1, 3)`**: Extract **1 to 3-word** phrases like:\n",
        "\n",
        "  * \"machine learning\"\n",
        "  * \"data analysis\"\n",
        "  * \"deep learning model\"\n",
        "* **`stop_words='english'`**: Removes common filler words like ‚Äúand,‚Äù ‚Äúis,‚Äù ‚Äúthe.‚Äù\n",
        "* **`top_n=15`**: Limits the output to the **15 most important phrases**.\n",
        "\n",
        "### üß™ Example:\n",
        "\n",
        "#### Input `text` (from resume):\n",
        "\n",
        "```\n",
        "I have experience with Python, machine learning, TensorFlow, Keras, and scikit-learn. I built models for classification, regression, and time series forecasting.\n",
        "```\n",
        "\n",
        "#### Output:\n",
        "\n",
        "```python\n",
        "['machine learning', 'TensorFlow', 'scikit-learn', 'time series forecasting', 'regression']\n",
        "```\n",
        "\n",
        "This is useful for:\n",
        "\n",
        "* Auto-tagging resumes with **extracted skills**\n",
        "* Matching resumes to job descriptions\n",
        "* Showing **strength areas** of a candidate\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Part 2: **Semantic Match with Job Description**\n",
        "\n",
        "```python\n",
        "def calculate_similarity(text1, text2):\n",
        "    emb1 = model.encode(text1, convert_to_tensor=True)\n",
        "    emb2 = model.encode(text2, convert_to_tensor=True)\n",
        "    score = util.pytorch_cos_sim(emb1, emb2).item()\n",
        "    return round(score * 100, 2)\n",
        "```\n",
        "\n",
        "### ‚úÖ Purpose:\n",
        "\n",
        "To **quantify how similar** two pieces of text are ‚Äî e.g., a resume and a job description ‚Äî based on their **semantic meaning**, **not just keywords**.\n",
        "\n",
        "### ‚öôÔ∏è How It Works:\n",
        "\n",
        "* Uses a `SentenceTransformer` model (e.g., MiniLM) to convert both texts into **dense vector embeddings**.\n",
        "* Then computes **cosine similarity** between them.\n",
        "* Converts it into a **percentage (0 to 100)** to indicate how close the match is.\n",
        "\n",
        "### üß™ Example:\n",
        "\n",
        "#### Input:\n",
        "\n",
        "```python\n",
        "resume_text = \"Experienced with React, Flask, and Docker for building full-stack apps.\"\n",
        "job_description = \"We are hiring a developer with strong knowledge of Flask and API development.\"\n",
        "```\n",
        "\n",
        "#### Output:\n",
        "\n",
        "```python\n",
        "calculate_similarity(resume_text, job_description)\n",
        "# ‚ûú 85.67\n",
        "```\n",
        "\n",
        "This is very helpful for:\n",
        "\n",
        "* Ranking applicants\n",
        "* Filtering resumes\n",
        "* Visualizing job fit scores\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Part 3: **Degree, Company & College Keyword Lists**\n",
        "\n",
        "```python\n",
        "DEGREE_KEYWORDS = [\n",
        "    \"Bachelor\", \"Bachelors\", \"B.Tech\", \"B.E\", \"BE\", \"BS\", \"BSc\",\n",
        "    \"Master\", \"M.Tech\", \"M.E\", \"MS\", \"MSc\", \"PhD\", \"Diploma\"\n",
        "]\n",
        "COMPANY_KEYWORDS = [\"Technologies\", \"Solutions\", \"Labs\", \"Systems\", \"Inc\", \"LLC\", \"Ltd\", \"Corporation\"]\n",
        "COLLEGE_KEYWORDS = [\"Institute\", \"University\", \"College\", \"School\", \"Academy\"]\n",
        "```\n",
        "\n",
        "### ‚úÖ Purpose:\n",
        "\n",
        "These are **reference keyword lists** used for rule-based **named entity extraction**, especially when identifying:\n",
        "\n",
        "1. **Degrees**: Useful for extracting education qualifications from resumes.\n",
        "2. **Company names**: Helps detect work experience affiliations.\n",
        "3. **College/institution names**: Helps isolate educational institutions.\n",
        "\n",
        "### üß™ Examples:\n",
        "\n",
        "#### üìò DEGREE\\_KEYWORDS\n",
        "\n",
        "Resume line:\n",
        "\n",
        "```\n",
        "Completed B.Tech in Computer Science from MIT WPU.\n",
        "```\n",
        "\n",
        "‚Üí Detected: `\"B.Tech\"` ‚Üí **degree**\n",
        "\n",
        "#### üè¢ COMPANY\\_KEYWORDS\n",
        "\n",
        "Resume line:\n",
        "\n",
        "```\n",
        "Worked at Turing Technologies as a backend intern.\n",
        "```\n",
        "\n",
        "‚Üí Contains `\"Technologies\"` ‚Üí **company**\n",
        "\n",
        "#### üè´ COLLEGE\\_KEYWORDS\n",
        "\n",
        "Resume line:\n",
        "\n",
        "```\n",
        "Graduated from Stanford University with an MS in AI.\n",
        "```\n",
        "\n",
        "‚Üí Detected: `\"University\"` ‚Üí **educational institute**\n",
        "\n",
        "These keyword lists can be used in your parsing logic like:\n",
        "\n",
        "```python\n",
        "for word in text.split():\n",
        "    if any(degree in word for degree in DEGREE_KEYWORDS):\n",
        "        print(\"Found a degree:\", word)\n",
        "```\n",
        "\n",
        "Or with Spacy's NER to **post-process and label entities** more accurately using these keyword hints.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Final Summary:\n",
        "\n",
        "| Component                          | Purpose                                          | Example Output                                                    |\n",
        "| ---------------------------------- | ------------------------------------------------ | ----------------------------------------------------------------- |\n",
        "| `extract_skills_auto(text)`        | Extract top 15 skill-like phrases                | `['machine learning', 'scikit-learn', 'time series forecasting']` |\n",
        "| `calculate_similarity(resume, jd)` | Measure match % between resume & job description | `86.23`                                                           |\n",
        "| `DEGREE_KEYWORDS`, etc.            | Help detect education, companies, and colleges   | `\"B.Tech\"`, `\"Technologies\"`, `\"University\"`                      |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "sNWSLqG4JCM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Automatic Skill Extraction ----------\n",
        "def extract_skills_auto(text, top_n=15):\n",
        "    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 3), stop_words='english', top_n=top_n)\n",
        "    return [kw[0] for kw in keywords]\n",
        "\n",
        "# ---------- Semantic Match with JD ----------\n",
        "def calculate_similarity(text1, text2):\n",
        "    emb1 = model.encode(text1, convert_to_tensor=True)\n",
        "    emb2 = model.encode(text2, convert_to_tensor=True)\n",
        "    score = util.pytorch_cos_sim(emb1, emb2).item()\n",
        "    return round(score * 100, 2)\n",
        "\n",
        "# ---------- DEGREE + ORGANIZATION ENHANCED EXTRACTION ----------\n",
        "DEGREE_KEYWORDS = [\n",
        "    \"Bachelor\", \"Bachelors\", \"B.Tech\", \"B.E\", \"BE\", \"BS\", \"BSc\",\n",
        "    \"Master\", \"M.Tech\", \"M.E\", \"MS\", \"MSc\", \"PhD\", \"Diploma\"\n",
        "]\n",
        "COMPANY_KEYWORDS = [\"Technologies\", \"Solutions\", \"Labs\", \"Systems\", \"Inc\", \"LLC\", \"Ltd\", \"Corporation\"]\n",
        "COLLEGE_KEYWORDS = [\"Institute\", \"University\", \"College\", \"School\", \"Academy\"]"
      ],
      "metadata": {
        "id": "JW6YvHY68Vy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üîß How `kw_model.extract_keywords()` Uses Embeddings\n",
        "\n",
        "At the heart of **KeyBERT** is the idea of **semantic similarity** ‚Äî it uses **sentence embeddings** to determine how relevant a candidate keyword is to the whole text.\n",
        "\n",
        "### ‚úÖ Here‚Äôs what happens under the hood:\n",
        "\n",
        "1. **Step 1: Convert the full input text into an embedding** (using a model like `all-MiniLM-L6-v2`).\n",
        "2. **Step 2: Generate candidate phrases** (1‚Äì3 words) using n-gram extraction.\n",
        "3. **Step 3: Embed each candidate phrase** the same way.\n",
        "4. **Step 4: Compute cosine similarity** between the full-text embedding and each phrase embedding.\n",
        "5. **Step 5: Rank the phrases** by similarity score ‚Äî highest scoring phrases are most representative.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Example Breakdown\n",
        "\n",
        "### Input:\n",
        "\n",
        "```text\n",
        "\"I have experience with Python, machine learning, TensorFlow, Keras, and scikit-learn. I built models for classification, regression, and time series forecasting.\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Step 1: Convert the full text into an embedding vector\n",
        "\n",
        "```python\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "text_embedding = model.encode(text)  # Output: a 384-dimensional vector\n",
        "```\n",
        "\n",
        "The embedding might look like:\n",
        "\n",
        "```python\n",
        "[0.035, -0.112, 0.208, ..., 0.076]  # size: (384,)\n",
        "```\n",
        "\n",
        "This is a **dense semantic representation** of the entire resume text.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Step 2: Extract candidate phrases (using n-grams)\n",
        "\n",
        "From the text, KeyBERT may extract candidates like:\n",
        "\n",
        "```\n",
        "[\"Python\", \"machine learning\", \"TensorFlow\", \"regression\", \"classification\", \"time series forecasting\", \"scikit-learn\", \"built models\"]\n",
        "```\n",
        "\n",
        "These are **1- to 3-word phrases** extracted using simple syntactic methods (not yet embeddings).\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Step 3: Convert each candidate phrase into its own embedding\n",
        "\n",
        "```python\n",
        "phrase = \"machine learning\"\n",
        "phrase_embedding = model.encode(phrase)\n",
        "```\n",
        "\n",
        "Output (again a 384-dim vector):\n",
        "\n",
        "```python\n",
        "[0.051, -0.097, 0.201, ..., 0.089]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Step 4: Compare phrase embedding to full-text embedding using cosine similarity\n",
        "\n",
        "```python\n",
        "from sentence_transformers import util\n",
        "score = util.cos_sim(phrase_embedding, text_embedding)\n",
        "```\n",
        "\n",
        "This will yield a score like:\n",
        "\n",
        "```python\n",
        "tensor([[0.93]])\n",
        "```\n",
        "\n",
        "Which means: \"machine learning\" is **very similar** to the main idea of the resume.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Step 5: Rank and return top N keywords\n",
        "\n",
        "KeyBERT sorts all phrases by similarity score and returns:\n",
        "\n",
        "```python\n",
        "['machine learning', 'TensorFlow', 'scikit-learn', 'time series forecasting', 'regression']\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Why Embeddings Matter Here\n",
        "\n",
        "* Instead of looking for **exact matches** (like regex), embeddings let you **understand meaning**.\n",
        "* Even if the resume says \"time series model\" and the job says \"forecasting\", the **semantic link** is detected.\n",
        "\n",
        "---\n",
        "\n",
        "## üß∞ Summary Table\n",
        "\n",
        "| Stage                | Input                | Output                 |\n",
        "| -------------------- | -------------------- | ---------------------- |\n",
        "| Sentence Embedding   | Full resume text     | Dense vector (384 dim) |\n",
        "| Candidate Extraction | n-gram phrases (1‚Äì3) | List of phrases        |\n",
        "| Phrase Embedding     | Each phrase          | 384-dim vector         |\n",
        "| Similarity Score     | Phrase vs Text       | Cosine similarity      |\n",
        "| Final Output         | Ranked phrases       | Top-N relevant skills  |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZfZB9pB_JBA_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QKkv48IYKYCa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}